# Installation

```
$ cd build
$ cmake .
$  cmake --build .
```

# Usage

## Training
```
$ JXSG_NN [path/to/config.json]
$ JXSG_NN ../config/MNIST.json
```



## Resources
```
https://steemit.com/ai/@ralampay/training-a-neural-network-a-numerical-example-part-1
http://dprogrammer.org/gradient-descent-algorithm
http://mccormickml.com/2014/03/04/gradient-descent-derivation/
https://www.youtube.com/watch?v=L_PByyJ9g-I&t=2439s
https://www.youtube.com/watch?v=z8DY5DndmxI
http://www.1-4-5.net/~dmm/ml/mse.pdf

Activation Functions
https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6

SoftMax and Entropy Cost Function
https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/
http://cs231n.github.io/linear-classify/#softmax
https://stats.stackexchange.com/questions/235528/backpropagation-with-softmax-cross-entropy
https://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network

Links for Ideas
https://www.youtube.com/watch?v=zXntbSlqjds
https://www.youtube.com/watch?v=z8DY5DndmxI
https://github.com/DenysRomanenko/DigitRecognition/blob/master/DigitRecognition/main.cpp
https://github.com/BobbyAnguelov/NeuralNetwork

https://en.wikipedia.org/wiki/Vanishing_gradient_problem